{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "https://developers.google.com/machine-learning/gan/ <br>\n",
    "https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#generative-adversarial-network-gan <br>\n",
    "https://github.com/llSourcell/Pokemon_GAN/blob/master/Generative%20Adversarial%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"img/genvsdisc.jpg\" width=50%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "\n",
    "- Aim to model how the data is generated. The model tries to captue the joint probability distribution $P(X,Y)$ or $P(X)$ when there are no labels provided.  \n",
    "\n",
    "### Discriminative Model \n",
    "\n",
    "- Aim to model the mapping between input $X$ to the labels $Y$. In terms of probability the model tries capturing $P(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods in ML\n",
    "\n",
    "- **Generative Methods**\n",
    "    - model class-conditional probability distributions functions and prior probabilites\n",
    "    - random sampling generates synthetic data points -> \"generative\" property\n",
    "    - example models: Gaussians, Naive Bayes , Mixture of Gaussians, Hidden Markov Models (HMM), Markov Random Fields, Bayesian Networks\n",
    "    \n",
    "- **Discriminative Methods** \n",
    "    - directly estimate posterior distribution\n",
    "    - does not try to model underlying probability distribution\n",
    "    - highly based on training data quality -> mapping input to output\n",
    "    - example models: Logistic regression, SVM, Neural Networks, Nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discriminative Learning**\n",
    "\n",
    "    Goal is learning $p(y|x)$. Probability function for the label $y$ given the input $x$.\n",
    "    \n",
    "- **Generative Learning**\n",
    "\n",
    "    Model $p(y)$, $p(x|y)$ first, then derive posterior \n",
    "    $p(x|y)=\\frac{p(y|x)p(y)}{p(x)}$<br>\n",
    "    Goal here is modeling the probabilty function for the labels $y$ and the probability for our input $x$ given our label $y$.\n",
    "    We also model $p(x,y) = p(y)p(x|y)$ through this giving us the joint probability distribution for input $x$ and label $y$ occuring at the same time.\n",
    "    This represents the underlying data structure.\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply illustrating the differnt modeling approaches yields:\n",
    "- the discriminative approach models the boundary between the different data points based upon the labels/classes\n",
    "- the generative approach models the data distribution depending on the data points and their label combined.\n",
    "<div><img src=\"img/gen_vs_disc.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of a Generative Adversarial Network (Goodfellow et al. 2014)\n",
    "<div><img src=\"img/GANConcept.png\" width=50%></div>\n",
    "\n",
    "The **Generator** draws some random parameters/noise denoted as $z$ from a source of randomness e.g. normal distribution and applies a function $f$ onto to it such that $\\hat x = f(z)$ where $ \\hat x$ is the output of the generator network. <br>\n",
    "The **Discriminator** is a binary classifier with the whole purpose being to differentiate between the real input $x$ and the generator input $\\hat x$.<br>\n",
    "The **Loss function** / Objective function can be stated respectively, for the generator we can state $\\min_G{p(y=\\text{fake}|\\hat x)}$ and for the discriminator we want to $\\max_D{p (y=\\text{fake}|\\hat x)}$ and $\\max_D{p (y=\\text{true}|x)}$.<br>\n",
    "Seen intuitively the generator is trying to fool the discriminator with its output $\\hat x$ to believe that it is a true data point while the discriminator is trying to maximize the accuracy by which it can determine if the input comes from our true data points $x$ or is a fake $\\hat x $ by the generator. We can formulate a combined value function using the log-likelihood as follows\n",
    "$$\\begin{aligned}\n",
    "\\min_G \\max_D L(D, G) \n",
    "& = \\mathbb{E}_{x \\sim p_{r}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))] \\\\\n",
    "& = \\mathbb{E}_{x \\sim p_{r}(x)} [\\log D(x)] + \\mathbb{E}_{x \\sim p_g(x)} [\\log(1 - D(x)]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are really only 5 components to think about:\n",
    "- $X:$ The original, genuine data set described by $p_r$\n",
    "- $I:$ The random noise that goes into the generator as a source of entropy described by $p_z$\n",
    "- $G:$ The generator which tries to copy/mimic the original data set which output can be described with $p_g$\n",
    "- $D:$ The discriminator which tries to tell apart $G$’s output from $X$\n",
    "- Backpropagation / Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training** <br/>\n",
    "*One training step consists of:*\n",
    "- Sample a mini batch of **m** noise vectors {$z^{1}$, $z^{2}$...., $z^{m}$}\n",
    "- Sample a mini batch of **m** training examples {$x^{1}$, $x^{2}$...., $x^{m}$}\n",
    "- Update **D** by doing one gradient descent step on its loss function: <br/>\n",
    "   $$J_{D}= -\\frac{1}{m} \\sum_{m}^{i=1} \\left [ log D(x^{(i)}) +   log(1- D(G(z^{(i)})))\\right ]$$\n",
    "- Update **G** by doing one gradient descent step on its loss function: <br/>\n",
    "   $$J_{G}= -\\frac{1}{m} \\sum_{m}^{i=1} \\left [ log D(G(z^{(i)}))\\right ]$$ \n",
    "\n",
    "\n",
    "**Training Math** <br>\n",
    "\n",
    "<div><img src=\"img/GAN_probs.png\" width =30%></div>\n",
    "\n",
    "Based upon our Loss function\n",
    "$$L(G, D) = \\int_x \\bigg( p_{r}(x) \\log(D(x)) + p_g (x) \\log(1 - D(x)) \\bigg) dx$$\n",
    "we can derive following statements:\n",
    "- $\\mathbb{E}_{x \\sim p_{r}(x)} [\\log D(x)]$ has no impact on the Generator $G$ during gradient descent\n",
    "\n",
    "Without proof the optimal value for $D$ based on the necessity $\\frac{d f(D(x))}{d D(x)} = 0$ of $f(x) = p_r(x)\\log(D(x))+p_g(x)\\log(1−D(x))$\n",
    "$$D^*(x) = \\frac{p_{r}(x)}{p_{r}(x) + p_g(x)} \\in [0, 1]$$ \n",
    "Once the generator is trained to its optimal, $p_g$ gets very close to $p_r$. When $p_g = p_r$, $D∗(x)$ becomes $1/2$.\n",
    "\n",
    "For the global optimum when $G$ and $D$ take the optimal value \n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(G, D^*) \n",
    "&= \\int_x \\bigg( p_{r}(x) \\log(D^*(x)) + p_g (x) \\log(1 - D^*(x)) \\bigg) dx \\\\\n",
    "&= \\log \\frac{1}{2} \\int_x p_{r}(x) dx + \\log \\frac{1}{2} \\int_x p_g(x) dx \\\\\n",
    "&= -2\\log2\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Representation of Loss Function $L$**\n",
    "\n",
    "What we want to show with this proof is how the Loss function of the GAN Network is related to the Jason-Shannon divergence.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D_{JS}(p_{r} \\| p_g) \n",
    "=& \\frac{1}{2} D_{KL}(p_{r} || \\frac{p_{r} + p_g}{2}) + \\frac{1}{2} D_{KL}(p_{g} || \\frac{p_{r} + p_g}{2}) \\\\\n",
    "=& \\frac{1}{2} \\bigg( \\log2 + \\int_x p_{r}(x) \\log \\frac{p_{r}(x)}{p_{r} + p_g(x)} dx \\bigg) + \\\\& \\frac{1}{2} \\bigg( \\log2 + \\int_x p_g(x) \\log \\frac{p_g(x)}{p_{r} + p_g(x)} dx \\bigg) \\\\\n",
    "=& \\frac{1}{2} \\bigg( \\log4 + L(G, D^*) \\bigg)\n",
    "\\end{aligned}$$\n",
    "\n",
    "thus $ L(G, D^*) = 2D_{JS}(p_{r} \\| p_g) - 2\\log 2$ and following that the best Generator $G^*$, meaing JS-divergence becomes $0$ and with that that $p_g = p_r$ will result in $L(G^*, D^*) = -2\\log2$\n",
    "\n",
    "\n",
    "**Important Discoveries**\n",
    "\n",
    "- Batch normalization is a must in both networks.\n",
    "- Fully hidden connected layers are not a good idea.\n",
    "- Avoid pooling, simply stride your convolutions!\n",
    "- ReLU activations are your friend (almost always).\n",
    "- Vanilla GANs could work on simple datasets, but DCGANs are far better.\n",
    "- DCGANS are solid baseline to compare with your fancy new state-of-the-art GAN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Neural Networks\n",
    "<div><img src=\"img/DCGAN.png\" width=50%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deconvolution Network cuppled with a CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with GANS\n",
    "\n",
    "**Nash Equilibrium** \n",
    "\n",
    "Two models are trained simultaneously to find a Nash equilibrium to a two-player non-cooperative game. However, each model updates its cost independently with no respect to another player in the game.\n",
    "\n",
    "Suppose one player takes control of $x$ to minimize $f_1(x)=xy$, while at the same time the other player constantly updates $y$ to minimize $f_2(y)=−xy$.\n",
    "Because $\\frac{∂f_1}{∂x}=y$ and $\\frac{∂f_2}{∂y}=−x$, we update $x$ with $x−η⋅y$ and $y$ with $y+η⋅x$ simulitanously in one iteration, where $η$ is the learning rate. Once $x$ and $y$ have different signs, every following gradient update causes huge oscillation and the instability gets worse in time.\n",
    "<div><img src=\"img/nash_equilibrium.png\" width=40%></div>\n",
    "\n",
    "**Low Dimensional Support**\n",
    "\n",
    "This problem is based on the assumption that every realworld data set $p_r$ only appears to have artifically high dimensions. This means it can be represented in a lower dimenional manifold. In simple terms your input data for example being vectors $x \\in \\mathbf{R}^n$ in n-dimensional space can be represented with a lower dimensionality by the latent space $Z$ by $z \\in \\mathbf{R}^m$ with the strict condition $m < n$. Because the generator is pretty much upscaling a random input of lower dimension to $p_g$ and the input data set $p_r$ lies in lower dimension itself there will always be a discriminator $D$ that will perfectly seperate real and fake samples becasue $D$ operates on the input dimensions $n$. Every added dimension gives the discriminator better \"options\" (more space) to be able to draw the boarder.\n",
    "<br> I highly recommend checking out the full post for this <a href=\"https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#low-dimensional-supports\"> here </a>. <br> \n",
    "\n",
    "**Vanishing Gradient**\n",
    "\n",
    "Suppose our discriminator $D$ is perfect then $D(x) = 1, \\enspace \\forall x \\in p_r$ and $D(x) = 0,  \\enspace \\forall x \\in p_g$. Then the Loss function $L$ will become $0$ and so do the gradients. <br>\n",
    "As a result, training a GAN faces a dilemma:\n",
    "\n",
    "- If the discriminator behaves badly, the generator does not have accurate feedback and the loss function cannot represent the reality.\n",
    "- If the discriminator does a great job, the gradient of the loss function drops down to close to zero and the learning becomes super slow or even jammed.\n",
    "This dilemma clearly is capable to make the GAN training very tough.\n",
    "\n",
    "**Mode Collapse** \n",
    "\n",
    "This is a problem concerning the generator network. Suppose the generator collapses to a setting where the output is fooling the discriminator most of the times but is in a low variety space (presenting the same generator output with only slight changes all the time). Mode collapse describes the problem that the model is only reproducing images or outputs based on a single class/label in the inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Tasks:\n",
    "- Kernel Mapping, Manifold Learning, Multi-dimensional Scaling (MDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

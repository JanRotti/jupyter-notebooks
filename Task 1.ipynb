{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept of Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class NeuralNetwork:   \n",
    "    # y is the label for the given input while x is the input\n",
    "    def __init__(self,x,y):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],3) # number of neurons in hidden layer = 3 \n",
    "        self.weights2 = np.random.rand(3,1)\n",
    "        self.y = y\n",
    "        self.output = np.zeros(y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input,self.weights1)) # +self.biases1)\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1,self.weights2)) # +self.biases2)\n",
    "        return self.layer2\n",
    "    \n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        # squared loss function L = (realLabel - outputLabel)Â²\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.input = X\n",
    "        self.output = y\n",
    "        self.output = self.feedforward()\n",
    "        self.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration # 0\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.81167371]\n",
      " [0.78000056]\n",
      " [0.78591139]\n",
      " [0.74180477]]\n",
      "Loss: \n",
      "0.32583055233690406\n",
      "\n",
      "\n",
      "for iteration # 100\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.52257965]\n",
      " [0.54302438]\n",
      " [0.49015109]\n",
      " [0.4761284 ]]\n",
      "Loss: \n",
      "0.242140094318696\n",
      "\n",
      "\n",
      "for iteration # 200\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.44539614]\n",
      " [0.74866483]\n",
      " [0.61123158]\n",
      " [0.15612168]]\n",
      "Loss: \n",
      "0.10926548751440018\n",
      "\n",
      "\n",
      "for iteration # 300\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.16979296]\n",
      " [0.86415188]\n",
      " [0.8562819 ]\n",
      " [0.05671286]]\n",
      "Loss: \n",
      "0.01778890127703706\n",
      "\n",
      "\n",
      "for iteration # 400\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.11338454]\n",
      " [0.9073895 ]\n",
      " [0.90404589]\n",
      " [0.03405845]]\n",
      "Loss: \n",
      "0.007949981951066004\n",
      "\n",
      "\n",
      "for iteration # 500\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.08944691]\n",
      " [0.92643034]\n",
      " [0.92427582]\n",
      " [0.02523912]]\n",
      "Loss: \n",
      "0.004946101996035735\n",
      "\n",
      "\n",
      "for iteration # 600\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.07576545]\n",
      " [0.93745193]\n",
      " [0.93584432]\n",
      " [0.02047444]]\n",
      "Loss: \n",
      "0.0035469546193731337\n",
      "\n",
      "\n",
      "for iteration # 700\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.06671921]\n",
      " [0.9447892 ]\n",
      " [0.94349717]\n",
      " [0.0174503 ]]\n",
      "Loss: \n",
      "0.002749191857011943\n",
      "\n",
      "\n",
      "for iteration # 800\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.06020078]\n",
      " [0.9500994 ]\n",
      " [0.94901347]\n",
      " [0.01533966]]\n",
      "Loss: \n",
      "0.002237283883435683\n",
      "\n",
      "\n",
      "for iteration # 900\n",
      "\n",
      "Input : \n",
      "[[1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.05523019]\n",
      " [0.9541613 ]\n",
      " [0.95322096]\n",
      " [0.01377162]]\n",
      "Loss: \n",
      "0.0018823741409081047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each row is a training example, each column is a feature  [X1, X2, X3]\n",
    "X=np.array(([1,1,1],[0,1,1],[1,0,1],[0,0,1]), dtype=float)\n",
    "y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "NN = NeuralNetwork(X,y)\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "    if i % 100 ==0: \n",
    "        print (\"for iteration # \" + str(i) + \"\\n\")\n",
    "        print (\"Input : \\n\" + str(X))\n",
    "        print (\"Actual Output: \\n\" + str(y))\n",
    "        print (\"Predicted Output: \\n\" + str(NN.feedforward()))\n",
    "        print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.feedforward())))) # mean sum squared loss\n",
    "        print (\"\\n\")\n",
    "  \n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Binary Step Function (activation function)\n",
    "def binstep(x):\n",
    "    return int(x>=0)\n",
    "\n",
    "# Perceptron\n",
    "x = np.array([1,0,-1])\n",
    "w = np.array(np.random.rand(3))\n",
    "# bias term random in (-1,1]\n",
    "b = np.random.random() *2 -1\n",
    "y = binstep(np.matmul(x.T,w) + b)\n",
    "\n",
    "# Defining 'Neuron' Function\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept of Neural Networks \n",
    "![title](img/NN.png)\n",
    "What the NN is actually doing is optimizing an approximated function onto a given data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- approximating the data given with a linear function \n",
    "- Perceptron enough (f(x)= wx+b)\n",
    "\n",
    "# Logistic Regression\n",
    "- used for binary output \n",
    "- fitting a logistic propability function\n",
    "\n",
    "![title](img/Regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Learning Problems \n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Rule Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "![title](img/Convolution.png)\n",
    "\n",
    "# Maxpooling\n",
    "- picking the maximum value in the filter size\n",
    "- in the above example with a 3x3 filter size the returned value is 2 \n",
    "\n",
    "# Flattening\n",
    "\n",
    "![title](img/Flattening.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and non-linear functions\n",
    "\n",
    "- The general form of a linear function is y = mx + b.\n",
    "- non-linear is everything else exceeding the linear definition\n",
    "\n",
    "# Question: How is non-linearity achieved in neural networks?\n",
    "- the activation functions introduce non-linearity to the neural network model (sigmoid,tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "What is ...\n",
    "Body: -combination of organ systems and tissue types into a single organism  \n",
    "\n",
    "Organ: -group of tissues which together perform similar functions (organspecific: functional/supportive tissue) \n",
    "\n",
    "Tissue: -ensamble of cells with similar function or group function\n",
    "\n",
    "Cell: -smallest unit of life\n",
    "\n",
    "Nucleus: -core of a cell surrounded by cytoplasm\n",
    "\n",
    "DNA(Deoxyribonucleic acid): -two polynuceleotide strands forming the typical helical shape containing AT or CG pairs and a alternating sugar-phosporus backbone structure, contains the gentic information of the organism (cytosine[C],thymine[T],adenine[A],guanine[G])\n",
    "\n",
    "RNA(Ribonucleic acid): -single stranded, transporting genetic information (coded building plan of proteins), important for protein synthesis\n",
    "\n",
    "Proteins: -polypeptides build with amino-acids with a vast function field like transporting signal-molecules etc.\n",
    "\n",
    "Cytoplasm: -everything inside the cell membrane except of the nucleus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Central-Dogma.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO functions mathematics\n",
    "\n",
    "> https://mbernste.github.io/files/notes/EvidenceLowerBound.pdf <br>\n",
    ">https://mbernste.github.io/posts/elbo/\n",
    "\n",
    "Given a probabilistic model $p(x,z)$ over observed random variables $X$ and latent random variable $Z$ then the evidence lower bound is a lower bound for the evidence, where the evidence is the log-likelihood of the observed data defined as $$\\log p(x) = \\log \\int p(x,z) dz$$\n",
    "Intuitively, if our model is correct, then the observed data should have a high probability\n",
    "under the model. Hence this quantity is “evidence” for the model. In the example of vae's we want to maximize the likelihood of $x$ being at the output. <br>\n",
    " Jensen’s Inequality states\n",
    "$$E[f(X)] ≤ f (E[X])$$\n",
    "where f is a convex function. Then, we derive the ELBO as a lower bound to the evidence as follows <br>\n",
    "First we introduce a new distribution $q(z)$ over the latent variable $Z$. Then we can define out lower bound as\n",
    "\n",
    "$$\\begin{aligned} \\log p(x) &= \\log \\int p(x,z) dz \\\\\n",
    "&= \\log \\int p(x,z) \\frac{q(z)}{q(z)} dz \\\\\n",
    "&= \\log (E_{Z∼q} [\\frac{p(x, Z)}{q(z)}] )\\\\\n",
    "&≥ E_{Z∼q} log \\frac{p(x, Z)}{q(z)}\\\\\n",
    "&= E_{Z∼q} log p(x, Z) − E_{Z∼q} \\log q(z) \\end{aligned}$$\n",
    "\n",
    "*Jensen Inequality is what we are using to turn the log term inside the expectation term.*\n",
    "\n",
    "$$\\begin{aligned} KL(q(z) \\ \\mid \\mid p(z \\mid x)) &:= E_{Z \\sim q}\\left[\\log \\frac{q(Z)}{p(Z \\mid x)} \\right] \\\\\n",
    "&= E_{Z \\sim q}\\left[\\log q(Z) \\right] - E_{Z \\sim q}\\left[\\log \\frac{p(x, Z)}{p(x)} \\right]\\\\\n",
    "&= E_{Z \\sim q}\\left[\\log q(Z) \\right] - E_{Z \\sim q}\\left[\\log p(x, Z) \\right] + E_{Z \\sim q}\\left[\\log p(x) \\right]\\\\\n",
    "&= \\log p(x) - E_{Z\\sim q} \\left[\\log \\frac{p(x, Z)}{q(z)}\\right]\\\\\n",
    "&= \\text{evidence} - \\text{ELBO}\\end{aligned}$$\n",
    "\n",
    "The difference between the evidence and the ELBO is precisely the KL-divergence between $p(z | x)$ and $q(z)$.\n",
    "This tells us one thing: If we want to approximate the log-likelihood of the data distribution given as $\\log p(x)$ we can do that by calculating the ELBO using a second distribution $q(z)$ such that we only need to observe joint probabilities and we know the error that is the KL-divergence of both distributions. This also yields that our approximation is **always** lower than the real evidence, suggesting our probabilistic model is always better than we observe with ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distribution vs Gaussian Distribution\n",
    "\n",
    "The normal distribution and the gaussian distribution describe the same.\n",
    "It is a continuous probability distribution that is symmetrical on both sides of the mean. The area under the normal distribution curve represents probability and the total area under the curve sums to one.\n",
    "- special case standard normal distribution with $\\mu = 0$ and $\\sigma =1$\n",
    "\n",
    "Example:\n",
    "When getting the IQ-score of 10.000 randomly selected people the distribution of the count of IQ's will capture the well known bell curve. Many natural occuring features can be statistically represented using a shifted normal distribution. \n",
    "![normal](img/gaussian-normal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM for clustering?\n",
    "\n",
    "The short answer is no!\n",
    "SVM is an supervised learning algorithm while clustering is unsupervised. This means the problem strcutur for both approaches are fundamentally different. <br>\n",
    "In SVM the class labels are provided and *known* and the problem tackled is seperating these classes as good as possible for example with a line (Linear SVM).\n",
    "\n",
    "> \"Minimize $\\|{\\vec {w}}\\|$ subject to ${ y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\geq 1}$ for $i=1,\\ldots ,n.$\" given $x_i \\in X$ and $y_i \\in Y$ with $X$ representing the input data and $Y$ being the respective labels.\n",
    "\n",
    "In clustering there are no class labels provided, the seperation/clustering process is based solely upon the input data and there inherent properties with respect to the similarity/distance measure.\n",
    "\n",
    "\n",
    "> ${\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}}$ given $(x_1,x_2,\\ldots,x_n)$ being a set of observations, $k$ being the number of clusters $S = {S_1, S_2, \\ldots, S_k}$ and $\\mu_i$ being the mean of the points in one cluster.\n",
    "\n",
    "The core difference between both algorithms is the problem setting. While SVM tackles the problem of seperation of classes with a hyperplane, clustering tries to identify similar datapoints and assign to a cluster, which can be seen as class assignement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary and multinomial Classification\n",
    "\n",
    "<dev><img src=\"img/classification.png\" width=50%></dev>\n",
    "\n",
    "**Binary classification** is the process of grouping elements into *two classes* on the basis of some sort of classification rule. \n",
    "\n",
    "**Multinomial classification** is more commonly called multiclass classificaiton and aims to classify instances onto one of three or more classes. This is different to multi-label classification. In multinomial classification only *one* class can be assigned to each instance out of the set of given classes. Most algorithms can be transformed from binary classification to multinomial classfication trough some sort of strategy. *One-vs-Rest* approaches this with training one classifier per possible class label (e.g. 4 classes in dataset, than 4 classifiers are trained), where  each sample assigned to the class is considered positiv and every other sample negative in respect to the class the clssifier works on. *One-vs-One* tries resolving the problem of having a large number of classifiers by letting each classifier differentiate between two of the classes solely. Suppose we have 4 classes in a dataset, then we have classes $c_1$, $c_2$, $c_3$, $c_4$, the classifiers are then semantically defined by $c_1$ vs $c_2$ (1), $c_1$ vs $c_3$ (2), $c_1$ vs $c_4$ (3), $c_2$ vs $c_3$ (4), $c_2$ vs $c_4$ (5) and $c_3$ vs $c_4$ (6). Each number ($i$) represents one binary classification problem (the number of binary classification problems can be derived from $ \\frac{n* (n – 1)}{2}$ with $n$ being the number of classes.\n",
    "\n",
    "\n",
    "> https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "\n",
    "#### The implementation of a binary and multinomial classification using the perceptron model with the iris dataset can be found as ClassificationWithPerceptron.ipynb\n",
    "#### For an SVM implementation on binary classification for SVM/SVM for Binary Classification.ipynb\n",
    "\n",
    "# Math on binary classification with Logistic Regression \n",
    "The base concept here is Linear Regression but with a twist. This means we fit a hyperplane in the form of $$ h_{0} = w  x + b $$ (hypothesis of linear regression) to our dataset such that on class lies on one side of the hyperplane and the other lies on the other side. But instead of leaving with just the hyperplane we apply the Logistic Function $\\frac{1}{1-e^{-x}} $, also known as sigmoid function, onto the result of the linear equation (hyperplane) such that the hypothesis yields\n",
    "$$h_{1} = \\frac{1}{1-e^{-(w x+b)}} $$\n",
    "Now we can denote our cost function for the logistic regression model $$L(w,b) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log {h_0(x^{(i)})} + (1 - y^{(i)})\\log {(1-h_0(x^{(i)}))}$$\n",
    "with $m$ being the batch size and $y$ being the label given as $y \\in \\{0,1\\}$\n",
    "\n",
    "> for the math of derivation of the cost functions with regard to $w$ and $b$ check <a href=\"https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d\"> here </a>\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b} =  - \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - h_0(x^{(i)})) $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_j} =  - \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - h_0(x^{(i)}))*x^{(i)}_j $$\n",
    "\n",
    "and as matrix operation: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = - \\frac{1}{m} X^T[h_0(x) - y] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Combining several learners to enhance performance of machine learning model.\n",
    "\n",
    ">http://scholarpedia.org/article/Ensemble_learning#Model_Selection\n",
    "\n",
    "\n",
    "<dev><img src=\"img/Bagging-Boosting.png\" width=50%></dev>\n",
    "<dev><img src=\"img/bagging.jpg\" width=50%></dev>\n",
    "<dev><img src=\"img/boosting.jpg\" width=50%></dev>\n",
    "# *B*ootstrap *agg*regat*ing* (bagging) - parallel ensembling\n",
    "\n",
    "Produces several different training sets of the same size with replacement (meaning you can get the same sample twice in your dataset) and then build a model for each one using the same machine learning scheme and combining their predictions by voting for a nominal target or averaging for a numeric target. Bagging can be parallelized. Bootstrapping refers to the splitting of the dataset into different stands each containing an instance of the learning algorithm.\n",
    "\n",
    "<dev><img src=\"img/bagging.png\" width=50%></dev>\n",
    "\n",
    "# Boosting\n",
    "\n",
    "Weak learners are sequentially produced during the training phase. The performance of the model is improved by assigning a higher weightage to the previous, incorrectly classified samples.\n",
    "\n",
    "# Hopfield Networks \n",
    "\n",
    "> http://web.cs.ucla.edu/~rosen/161/notes/hopfield.html\n",
    "\n",
    "Every input dimension is linked to exactly one neuron. The other inputs of each neuron come from the output of all of the other neurons (but not itself). The weight matrix is symmetrical and there are no self_connections. The update in the network is iterative over every input.\n",
    "\n",
    "<div><img src=\"img/hopfield.jpg\" width=50%> </div>\n",
    "\n",
    "$S$ represents the State Values of the nodes. The weight matrix can be calculated from  $$w_{ij} = \\sum_{p=1}^P[2S_i−1][2S_j−1] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

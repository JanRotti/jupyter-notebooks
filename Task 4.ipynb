{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does backpropagation not work on random variables?\n",
    "\n",
    "The backpropagation algorithm is inherently based on the property for the whole model to be differentiable. During the updating step you compute the gradient backwards trough derivating the objective function with respect to the parameters we want to optimize. Because the sampling process itself is inherently stochstical and we can't directly assign a function that can be differentiated with respect to the parameters $\\mu$ and $\\sigma$ we need to find a way to outsource this stochastic process to an independent random variable. This usually come from the difficulty in obtaining a random sample, that is most commonly done in computers for in example looking at the state of certain bits as inputs to an underlying function. BUt mathematically there is a clever work around. This is where a property for normal distributions come into place. You can shift a normal distribution trough \n",
    "$$\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} \\text{, where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{I})$$\n",
    "such that you can now differentiate with respect to $\\mu$ anf $\\sigma$ by treating $\\epsilon$ as constant for each cycle of backpropagation and only $\\epsilon$ containing the unknown function for the process of random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO - Evidence Lower Bound\n",
    "Where evidence is nothing but the log-likelihood given our parameters $\\theta$ are fixed.\n",
    "\n",
    "> https://mbernste.github.io/posts/elbo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Grid-Search - Hyperparameter Optimization\n",
    "**Goal of Hyperparameter Optimization** is achieving optimal performance for a given model on a dataset. Grid-Seach is the most basic optimization scheme in this context. \n",
    "\n",
    "**What is the difference between Hyperparameters and Parameters?**\n",
    "- **Hyperparameter** : <br> \n",
    "> They are the parameters defining the external chracteristics of your model\n",
    "\n",
    "    *Hyperparameters related to structure:* number of hidden layers and neurons, dropout, initialization, activation function, number of kernels, striding etc. <br>\n",
    "    *Hyperparameters related to training algorithm:* learning rate, momentum, epochs, batch size etc. <br>\n",
    "- **Parameter** : \n",
    "> Define the internal characteristic of the model. These are the coefficients of the model that are actually optimized to fit the data.\n",
    " \n",
    "**Grid Seach** is an exhaustive searching through a manually defined search space. Each dimension of this search space represents one hyperparameter and each point (vector) in this space relates to one model configuration. The metric by which the configurations are evaluated and measured can be defined as parameter in itself (accuracy,precision,confusion matrix etc.) \n",
    "\n",
    "<div><img src=\"img/Grid_Search.png\" width=20%> </div>\n",
    "\n",
    "**Random Search**  utilizes random selection of the hyperparameter from the search space. This can be done similar to grid search with sets of descrete values but advanced to a continuous space. \n",
    "\n",
    "<div><img src=\"img/Random_Search.png\" width=20%> </div>\n",
    "\n",
    "\n",
    "**Bayesian Optimization** IF TIME !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Tools for Bounding Boxes\n",
    " - LabelMe: Web-based Image Annotation Tool, Free to Use, dataset is pretty much publicly available (privacy setting just lets the dataset disappear from the public listing, url consists)\n",
    " http://labelme2.csail.mit.edu/Release3.0/browserTools/php/browse_collections.php?username=janrotti\n",
    " - LabelImg: Python based open-source annotation tool \n",
    " https://github.com/tzutalin/labelImg\n",
    " - Diffgram: Web-based Image Annotation Tool, Free to Use, Set up in a project structure, dashboards to follow progress, working team can be managed, bit more comlicated in the beginning\n",
    " https://diffgram.com/home/dashboard?frame=0&"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMS + MSE ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of dimensionality\n",
    "> \"As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.\" <br>\n",
    "> \\- Charles Isbell, Professor and Senior Associate Dean, School of Interactive Computing, Georgia Tech\n",
    "\n",
    "Too many dimensions causes every observation in the dataset to appear equidistant from all the others. With this the difference and similarity between the data points gets obscured. \n",
    "\n",
    "The relationship between dimensions and dimension space is exponential.\n",
    "\n",
    "![Curse](img/curseofdimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1, L2 Regularization \n",
    "Both add a penalty term to the objective function. The name L1 and L2 regularizer extends the norm used for calculation respectively.\n",
    "Given a vector $w \\in \\mathbb{R}^n$ then \n",
    "$$\\begin{align}\\text{L1-norm is defined as:}\\quad ||w||_1 &= |w_1|+|w_2|+|w_3|+...+|w_{n-1}|+|w_n| \\\\\n",
    "\\text{L2-norm is defined as:}\\quad||w||_2 &= (|w_1|^2+|w_2|^2+|w_3|^2+...+|w_{n-1}|^2+|w_n|^2)^{\\frac{1}{2}}\\\\\n",
    "\\text{p-norm is defined as:}\\quad||w||_p &= (|w_1|^p+|w_2|^p+|w_3|^p+...+|w_{n-1}|^p+|w_n|^p)^{\\frac{1}{p}}\\end{align}$$\n",
    "\n",
    "Suppose our model is a linear regression model:\n",
    "$$ y = w_1x_1+w_2x_2+w_3x_3+...+w_nx_n$$\n",
    "\n",
    "then we define our loss functions as follows where $\\lambda$ is the regularisation parameter:\n",
    "$$\\begin{align}\\text{Basic Loss L:}\\quad L &= Error(y,\\hat y) \\\\\n",
    "\\text{Loss based on L1:}\\quad L1 &= Error(y,\\hat y)+ \\lambda \\sum_{i=1}^n |w_i|\\\\\n",
    "\\text{Loss based on L2:}\\quad L2 &= Error(y,\\hat y)+ \\lambda \\sum_{i=1}^n |w_i|^2\\end{align}$$\n",
    "\n",
    "In this case we defined our Error function to be squared error\n",
    "$$ Error = (y-\\hat y)^2$$\n",
    "This leads to the loss functions:\n",
    "$$\\begin{align}\\text{Basic Loss:}\\quad L &= (y-\\hat y)^2 \\\\\n",
    "\\text{Loss based on L1:}\\quad L1 &= (y-\\hat y)^2+ \\lambda \\sum_{i=1}^n |w_i|\\\\\n",
    "\\text{Loss based on L2:}\\quad L2 &= (y-\\hat y)^2+ \\lambda \\sum_{i=1}^n |w_i|^2\\end{align}$$\n",
    "\n",
    "The learning step is achieved with the backpropagation algorithm where $\\eta$ is the learning rate:\n",
    "$$ w_{new} = w - \\eta \\frac{ \\partial L}{ \\partial w}$$\n",
    "\n",
    "Now lets substitue the last term with the gradient of $L,L1 \\text{ and }L2$\n",
    "$$\\begin{align}\\text{L:}\\quad w_{new} &= w -\\eta *[2 x (wx+b-\\hat y)] \\\\\n",
    "\\text{L1:}\\quad w_{new} &= w - \\eta *[2x(wx+b-\\hat y)+\\lambda]\\\\\n",
    "\\text{L2:}\\quad w_{new} &= w - \\eta *[2x(wx+b-\\hat y)+2 \\lambda w]\\end{align}$$\n",
    "\n",
    "> During this step mathematically the derivative of the L1 term depends on the sign of $w$ but is neglected here with stating: $w >= 0$\n",
    "\n",
    "For simplicity we define $G := 2x(wx+b-\\hat y)$ and $\\eta = 1$. Then \n",
    "$$\\begin{align}\\text{L:}\\quad w_{new} &= w - G \\\\\n",
    "\\text{L1:}\\quad w_{new} &= w - G-\\lambda\\\\\n",
    "\\text{L2:}\\quad w_{new} &= w - G- 2 \\lambda w\\end{align}$$\n",
    "\n",
    "**Intuitions**\n",
    "\n",
    "Suppose using only $L$ during the training process. The parameter update depends only on the given weight value $w$ and the data $x$ as seen in $G$. Then with this backpropagation our $w$ is updated towards $w^*$ the optimal weight parameters for our given data. But this is not what we want the model to learn. The goal is to generalize the data to perform well even on unseen data. This is where the additional terms in $L1$ and $L2$ come into place. Now the update is shifted away from the optimal weight parameters $w^*$ with an offset. This can be thought about shifting the weights by a term that is not data driven in itself so it provides a basis for generalization. \n",
    "\n",
    "**L1 and L2**\n",
    "\n",
    "Looking at the two differnt penalty terms $\\pm \\lambda$ and $- 2 \\lambda w$ we can conclude for once that the L1 regularization want the weight values to be forced to 0. This will result in a feature reduction effect on the model because only the most general(dominant) or most meaningful features will influence the model, the weights of the other features are pushed to 0. The L2 regularization term does not inherit this property per se, but will force the magnitude and with this the impact of weights in general to be lower and has a similar effect on feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Dropout is a completly different form of regularization in contrast to L1 and L2 regularization. Instead of modifying the objective function we modify the network structure in itself. For each cycle of forward pass and backward pass we \"deactivate\" some neurons by synthetically setting the neurons to $0$ and not updating them in the backwards pass by a probability set beforehand. \n",
    "- Dropout roughly doubles the number of iterations necessary for convergence but less time for training is required due to less updates to parameters \n",
    "- Dropout encourages the network to generalize the data more by reducting the dependability of neurons\n",
    "- Dropout makes the model more robust "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of autoencoders and their applications\n",
    "\n",
    "\n",
    "**Applications**\n",
    "\n",
    "- dimensionality reduction\n",
    "- data denoising\n",
    "- feature extraction\n",
    "- image generation \n",
    "- sequence-to-sequence prediciton\n",
    "- recommondation systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions\n",
    "\n",
    "**Definition**\n",
    "\n",
    "A probability distribution is a statistical function describing all the possible values and likelihoods that a random variable can take.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu :& \\enspace\\text{mean}\\\\\n",
    "\\sigma :&\\enspace \\text{standard deviation}\\\\\n",
    "\\sigma^2 :&\\enspace \\text{variance}\\\\\n",
    "\\tau :& \\enspace \\text{precision}\n",
    "\\end{align*}\n",
    "$$\n",
    "normal probability density function\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}exp(- \\frac{1}{2} (\\frac{x-\\mu}{\\sigma})^2)\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{standard normal distribution} :&\\enspace \\mu = 0 ;\\enspace \\sigma = 1\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "**Bernoulli Distribution** two states (denoted as 0 and 1)\n",
    "\n",
    "$$ p(x) = \\begin{cases}\\begin{align}p \\quad x=&\\enspace 1 \\\\(1-p) x=&\\enspace 0 \\end{align}\\end{cases}$$\n",
    "**Uniform Distribution** every event in a given interval $[a,b]$ is equally likely. The best example of a uniform distribution is the roll of a regular dice.\n",
    "$$ p(x) = \\begin{cases}\\begin{align} \\frac{1}{b-a} \\enspace &\\text{for} \\enspace x \\ in  \\enspace[a,b]  \\\\ 0 \\enspace &\\text{otherwise} \\end{align}\\end{cases}$$\n",
    "<div><img src=\"img/Uniform.png\" width=20%></div>\n",
    "\n",
    "**Binomial Distribution** is advancing from the Bernoulli Distriution where $n=1$ and $n$ is the number of independent experiments performed. It describes the propability of a certain outcome $k$ depending on the probability of success and failure.\n",
    "$$ f(k,n,p) = {n \\choose k} p^k(1-p)^{n-k}$$\n",
    "<div><img src=\"img/binomial.png\" width=40%></div>\n",
    "\n",
    "**Poission Distribution** for events ocurring at random points in time and the number of events is at importance. Denoting $k$ being the number of occurences and $\\lambda$ being an distribution parameter that is equivalent to the expected value and also its variance. or in simpler terms $\\lambda$ is the mean number of occurences in the interval. We can also break down $\\lambda$ to $\\lambda = rt$ with $r$ being the number of occurences per unit of time and $t$ being time.\n",
    "$$ f(k,\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "<div><img src=\"img/poisson.png\" width=50%></div>\n",
    "\n",
    "**Gamma Distribution**\n",
    "given the shape-parameter $\\alpha$ and rate-parameter $\\beta$ we can denote with $\\Gamma (k) = (k -1)!$  denoting the Gamma function, $\\beta = \\frac{1}{\\theta}$ denoting the rate of events happening following the poisson process and $\\alpha = k$ being the number of events happening until which we are waiting for an expected event to occur.\n",
    "\n",
    "$$f(x,k,\\theta) = \\frac{x^{k - 1}e^{-\\frac{x}{\\theta}}}{\\theta^k \\Gamma (k)}$$\n",
    " \n",
    "<div><img src=\"img/gamma.png\" width=30%></div>\n",
    "\n",
    "**Exponential Distribution** is a special case of gamma distribution where $\\alpha = 1$ and $\\beta = \\lambda$\n",
    "\n",
    "$$ f(x,\\lambda) = \\lambda e^{-\\lambda x} $$\n",
    "\n",
    "<div><img src=\"img/exponential.png\" width=30%></div>\n",
    "\n",
    "**Chi-Square Distribution** is a special case fo gamma distribution denoted with $\\chi^2$ and the parameters $\\alpha,\\beta $ being $\\alpha = k/2$ and $\\beta= \\frac{1}{2} $\n",
    "\n",
    "$$ \\frac{1}{2^{k/2}\\Gamma(k/2)} x^{k/2-1}e^{-x/2}$$\n",
    "\n",
    "<div><img src=\"img/chi.png\" width=30%></div>\n",
    "\n",
    "**F-Distribution** or also called Fisher–Snedecor distribution and is used when we want to determine the ratio of the variances of two normally distributed populations. Or as used in ANOVA to examine the variation between several groups to the variation in the group itself. The formula itself is complicated in itself and depends on two parameters $d1$ and $d2$ which denote the degrees of freedom. Derived from this distribution is the F-Score\n",
    "\n",
    "<div><img src=\"img/f.png\" width=30%></div>\n",
    "\n",
    "**Student's t-Distribution** is most commonly used when the sample size is small and the standard deviation is unknown. If we take a sample of $n$ observations from a normal distribution, then the t-distribution with $\\nu =n-1$ degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term $\\sqrt {n}$\n",
    "\n",
    "<div><img src=\"img/student.png\" width=30%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous and discrete distributions \n",
    "| discrete     | continuous     | \n",
    "| :------------|:---------------|\n",
    "| binominal | uniform |\n",
    "| poisson | normal |\n",
    "| bernoulli| chi|\n",
    "||f|\n",
    "||gamma\n",
    "\n",
    "> there is a full list of distributions available here https://en.wikipedia.org/wiki/List_of_probability_distributions#Discrete_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler-divergence\n",
    "Given two probability distributions $P$ and $Q$ defined on the same probability space $\\chi$ then the Kullback-Leibler divergence from $Q$ to $P$ is defined as \n",
    "$$ KL(P || Q) = \\sum_{x \\in \\chi} P(x)*log(\\frac{P(x)}{Q(x)})$$\n",
    "\n",
    "Intuitively this yield that when the probability of an event $x$ from $P$ is large the probability for the same event $x$ in $Q$ has to be the similar for the KL-divergence to be small. In return if the difference in probability of the event $x$ in both distributions $P$ and $Q$ differ the KL-divergence is greater. \n",
    "In addition to that one property of the KL divergence is that $ KL(P || Q) \\neq KL(Q || P)$. This can also be shwon by looking at the logarithmic part of the KL-divergence formula. \n",
    "Suppose we have two events $x_1$ and $x_2$ where $P(x_1) > Q(x_1)$ and $P(x_2) < Q(x_2)$ then we can show that for both events $x_1$, $x_2$ the Kullback-Leibler divergence holds \n",
    "$$KL(P(x_1)||Q(x_1)) \\geq KL(P(x_2)||Q(x_2)) $$ \n",
    "Because of the asymmetrical properties the difference in distance measure KL-divergence from $Q$ to $P$ is inherently different to the KL-divergence from $P$ to $Q$. This is also shown in the figure below where $Q$ is the distribution that is minimized on the KL-divergence score based on both terms.\n",
    "<div><img src=\"img/KLdivergence.png\" width=60%></div>\n",
    "\n",
    "> simply spoken $KL(P || Q)$ is fitting $Q$ to $P$ from below and $KL(Q || P)$ if fitting from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen-Shannon-divergence\n",
    "\n",
    " Given two probability distributions $P$ and $Q$ defined on the same probability space $\\chi$ then the Jensen-Shannon divergence from $Q$ to $P$ is defined as \n",
    " \n",
    "$$JS(P || Q) = 1/2 * KL(P || M) + 1/2 * KL(Q || M) $$\n",
    "where $$M = 1/2 * (P + Q) $$\n",
    "\n",
    "The JS-divergence utilizes the KL-divergence to calculate a normalized score that is symmetrical in difference to KL itself. This means it holds\n",
    "\n",
    "$$JS(P(x)||Q(x)) = JS(P(x)||Q(x)) $$ \n",
    "\n",
    "The JS-divergence is bound between 0 meaning identical and 1 meaning maximally different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Jensen-Shannon is sometimes worse that Kullback-Leibler\n",
    "- $KL[q;p]$  kind of abhors regions where  $q(x)$  have non-null mass and  $p(x)$  has null mass\n",
    "- When trying to find approximations for a complex (intractable) distribution  $p(x)$  by a (tractable) approximate distribution  $q(x)$  you want to be absolutely sure that any  $x$  that would be very improbable to be drawn from  $p(x)$  would also be very improbable to be drawn from  $q(x)$ . That KL have this property is easily shown: there’s a  $q(x)log[q(x)/p(x)]$  in the integrand. When  $q(x)$  is small but  $p(x)$  is not, that’s ok. But when  $p(x)$ is small, this grows very rapidly if  $q(x)$  isn’t also small. So, if you’re choosing  $q(x)$  to minimize $ KL[q;p]$ , it’s very improbable that  $q(x)$  will assign a lot of mass on regions where $p(x)$  is near zero.\n",
    "\n",
    "<div><img src=\"img/Data_Example.jpg\"></div>\n",
    "\n",
    "The discussion linked below is especially helpful as to the explanation why the minimization of JS-divergence is not equal to the maximization of the log-likelihood\n",
    "\n",
    "https://stats.stackexchange.com/questions/405355/does-the-jensen-shannon-divergence-maximise-likelihood\n",
    "\n",
    ">The Jensen-Shannon divergence is the average of the two, so one can think of finding a minimum as \"a little bit of both\" (distributions), meaning something in between the maximum likelihood estimate and a \"simple explanation\" for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "\n",
    "The definition for clustering:<br>\n",
    "(1) Instances, in the same cluster, must be similar as much as possible;<br>\n",
    "(2) Instances, in the different clusters, must be different as much as possible;<br>\n",
    "(3) Measurement for similarity and dissimilarity must be clear and have the practical\n",
    "meaning;<br>\n",
    "\n",
    "There are two measures used in clustering algorithms. The first is distance and the other is similarity but the usecase depends on the kind of features. For quantitative features distance functions are used and for qualitative features.  \n",
    "<div><img src=\"img/distancefunctions.PNG\" width=40%>\n",
    "<img src=\"img/similarityfunctions.PNG\" width=40%></div> <br><br>\n",
    "\n",
    "Clustering algorithms can be categorized as follows containing the most commonly used algorithms.\n",
    "<div><img src=\"img/clusteringalgorithms.PNG\" width=33%><br></div>\n",
    "\n",
    "In the following section I will intruduce the top clustering algorithms widely used.\n",
    "\n",
    "**K-Means** is an algorithm in the category \"based on partition\" and can be better described as centroid-based clustering. The base idea is that you have a predefined number of clusters and you are updating the center of this clusters based on the center of the data points within a cluster.\n",
    "<div><img src=\"img/kmeansalgorithm.png\" width=50%></div>\n",
    "\n",
    "> There is a visualization of the K-Means Clustering algorithm here <br>\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "**Mean-Shift** clustering is a centorid-based algorithm two and expands k-means in a way such that initially the number of clusters does not have to be known. The algorithm starts with a given arrangement of random starting points. These starting points are overlayed with a kernel (for example in 2D a circle with radius $r$). In each iterations step each point is shifted towards regions with higher density of data points inside the kernel. This will result in points moving in regions with higher data point density. When the kernels of these points overlap the points are getting clustered together and the point with a higher density of datapoints inside its kernel is preserved.  \n",
    "I will explain the principle of **Clustering based on hierachy** but will not present a principal algorithm in this section. The core idea here is that every data point start of as its own cluster. In iterative steps the two most neighboring clusters are merged into one such that in the end you are left with a tree like structure with only one cluster at the top. This for example can be useful in grouping words together.\n",
    "\n",
    "<div><img src=\"img/HierarchicalClustering.svg\" width=50%></div>\n",
    "\n",
    "\n",
    "**Clustering based on Fuzzy-Theory** is based on the concept that the belong to a cluster does not have to be represented by a true ${0,1}$ label but instead can be modeled on a continuous scale $[0,1]$. This also means that a data point can be assigned to multiple clusters.  The most well known in this domain is Fuzzy K-Means. Here the updating step is slightly advanced because of the additional data points considered for each cluster such that a weight factor is introduced being the label value in the iteration step in question. \n",
    "\n",
    "**DBSCAN** is an algorithm in the field of **Clustering based on Density**. The concept of this is start you start on a datapoint, that has not yet been visited. Then the neighborhood of this point is extracted using a defined distance $\\epsilon$ and are assigned as neighborhood points. The next point is selected from this neighborhood based upon the distance measure and the step is repeated until no point not yet visited is found in the neighborhood group. If there is a sufficient enough points in the neighborhood it is assigned as cluster and the process start new from a random data point that is still not yet visited by the algorithm. \n",
    "\n",
    "**Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)** belongs to **Clustering based on Distribution**. The base concept is similar to K-Mean but instead of assigning the cluster centers with the fixed mean value we assign it to a probabilty distribution based upon the points in the cluster (you need to calculate the mean and standard-deviation). The optimization step is done by the expectation-maximization algorithm. The number of clusters has to be defined and randomly initialized parameters for the Gaussian distributions are generated.\n",
    "\n",
    "Given these Gaussian distributions for each cluster, compute the probability that each data point belongs to a particular cluster. The closer a point is to the Gaussian’s center, the more likely it belongs to that cluster. \n",
    "Based on these probabilities, we compute a new set of parameters for the Gaussian distributions such that we maximize the probabilities of data points within the clusters. We compute these new parameters using a weighted sum of the data point positions, where the weights are the probabilities of the data point belonging in that particular cluster. These steps are repeated iteratively until convergence, where the distributions don’t change much from iteration to iteration.\n",
    "\n",
    "\n",
    "> A Comprehensive Survey of Clustering Algorithms <br>\n",
    " https://link.springer.com/article/10.1007/s40745-015-0040-1 <br>\n",
    " In addition an article with the five important algorithms explained in detail <br>\n",
    " https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractually Strided Convolution, Transposed Convolution and Deconvolution\n",
    "\n",
    "Suppose the kernel $w$ defines a convolution whose forward and backward passes are computed by multiplying with $C$ and $C^T$ respectively, it also defines a transposed convolution whose forward and backward passes are computed by multiplying with $C^T$ and $(C^T)^T = C $ respectively. In practice zeros are inserted between the input pixels such that the movement of the kernel is slowed down in the convolution step.\n",
    "\n",
    "This deconvolution or fractually strided convolution is illustrated below, where blue maps are inputs and cyan maps are outputs. The darkened regions are the kernel overlay in the seperate convolution steps and the dotted lines are added zeros around the original input. \n",
    "\n",
    "![decon](img/2aSir.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
